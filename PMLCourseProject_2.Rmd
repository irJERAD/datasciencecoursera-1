---
title: "CourseProjectPracticalMachineLearning"
author: "mFriz"
date: "Monday, January 19, 2015"
output: html_document
---

## Data Acqusition and Data Cleaning

First I looked at the data set and observed different kinds of empty or useless values. Those will be dealt with by setting
```{r}
na.strings=c("NA", "", "#DIV/0!")
```
when reading in the .csv file:

To load the training data set, I put the corresponding .csv file into the current working directory. 
```{r}
data <- read.csv(file="pml-training.csv", header=TRUE, sep=",", na.strings=c("NA", "", "#DIV/0!"))                                                                   
```
How many rows and columns are there?
```{r}
nrow(data)
ncol(data)
```
```
summary(data)
```
There are a couple of variables with over 19215 NAs. This means less than 2.1% of the rows of such a variable have non-NAs in them. Although such variables may be helpful nontheless - considering there are 160 variables overall - they are removed from the dataframe. The following code will find the variables with too many NAs, i.e. more NAs than in "thresholdNA". 
```{r}
thresholdNA <- 19215
lvNotTooManyNAs <- vector(mode="logical", length=ncol(data)) 
```
lv stands for logical vector. As the name suggests, a value is TRUE if there are not too many NAs in a certain column. 
The following code creates the logical vector:
```{r}
for (i in 1:ncol(data)) {
    if (length(which(is.na(data[ , i]))) > thresholdNA) {
        lvNotTooManyNAs[i] <- FALSE
    } else {
        lvNotTooManyNAs[i] <- TRUE
    }
}
```

According to the logical vector, the data set is reduced, i.e. some columns removed. 
```{r}
data2 <- subset(data, select=lvNotTooManyNAs)
```

Let us look at data again. 
```{r}
ncol(data2)
```
```
summary(data2)
```

There are some columns, that should be of no interest, such as the indices, user_name, the time stamp columns as well as the window number and wheather or not a new window was used. These will be removed:
```{r}
data3 <- data2[ , -c(1:7)]
```
We are now down to 53 accelerometer variables. 

Finally, let us not forget to make classe a factor variable.
```{r}
data3$classe <- as.factor(data3$classe)
```

## Define Error Rate

This problem at hand is a classification problem. The overall accuracy will be used to assess the validity of the models.  


## Split Data into Training Set, Test Set

There are 19 thousand rows, so I consider the data set large and split into 60% training/ 20% validation/ 20% test. 
* The training set will be used to train the model.
* The validation set will be used for choosing the model. 
* The test set will only be used by the final model, in order to calculate the prediction error.

```{r}
library(caret)
set.seed(2343)
trainIndices <- createDataPartition(data3$classe, p = 0.60, list=FALSE)
trainSet <- data3[trainIndices, ]
tempSet <- data3[-trainIndices, ]
valIndices <- createDataPartition(tempSet$classe, p = 0.5, list=FALSE)
valSet <- tempSet[valIndices, ]
testSet <- tempSet[-valIndices, ]
```

## On the Training Set, Pick Features and Use Cross-Validation.

At this point, no further analysis of the features was done, i.e. all features will be used in the following. I note that I am aware of methods such as the principal component analysis. Removing more features will speed up the computation. Using cross-validation will slow it down. 

## On the Training Set, Pick Prediction Functions and Use Cross-Validation. 
There are countless models that can be used for classification problems. I will use the random forest model and the support vector machine model. 

### Setting up parallel processing
Before starting the training, I will set up some workers for parallel computation. 
```{r parallel}
library(doParallel)
numberCores <- detectCores()
cl <- makeCluster(numberCores - 1) # leave one core to not block the PC
registerDoParallel(cl)
getDoParWorkers()
```
Caret should be able to use these workers without the user doing anything. Note that 
allowParallel = TRUE is the default in trainControl(). 

### Random Forest Model
The random forest model will be used as the first model fit. The random forest model seems to be a good choice in most classification problems. See for example in "Do we need Hundreds of Classifiers to Solve Real World Classification Problems" in Journal of Machine Learning Reserach 15 (2014) 3133-3181. The result of that paper is that 

>"The best results are achieved by the parallel random forest (parRF t), implemented in R with caret, tuning the parameter mtry. The parRF t achieves in average 94.1% of the maximum accuracy over all the data sets (Table 5, lower part), and overcomes the 90% of the maximum accuracy in 102 out of 121 data sets. Its average accuracy over all the data sets is 82.0%, while the maximum average accuracy (achieved by the best classifier for each data set) is 86.9%. The random forest in R and tuned with caret (rf t) is slightly worse (93.6% of the maximum accuracy), although it achieves slightly better average accuracy (82.3%) than parRF t." 

To get an idea of the computation speed, I started a model fit using the pure randomForest() function from the randomForest library. The following model includes some cross-validation and an increased tuneLength:
```{r caretRf train, cache=TRUE}
set.seed(2343)
seeds <- vector(mode = "list", length = 50) # 50 = 7 * 7 +1
for(i in 1:49) seeds[[i]] <- sample.int(1000, 7)
seeds[[50]] <- sample.int(1000, 1)
set.seed(2343)
ptm1 <- proc.time() # Start clock
ctrl <- trainControl(method="cv", 
                     number = 7,
                     seeds=seeds)
modelFitCaretRf <- train(classe~., 
                                data=trainSet, 
                                method="rf",
                                trControl=ctrl,
                                tuneLength=7)
ptmDiff <- proc.time() - ptm1 # Stop clock
ptmDiff
modelFitCaretRf
```

### Support Vector Machine Model
Another strong model with regards to accuracy in classification problems is the support vector machine model. This will be used in the following. First I used directly kerlab's ksvm() with a support vector machine model with radial basis functions. This way I got an idea of the computation time. In the following, carets train() function is used, together with some cross-validation and an increased tuneLength. 
```{r}
library(kernlab)
```
```{r caretSvm train, cache=TRUE}
set.seed(2343)
seeds <- vector(mode = "list", length = 99) # 99 = 7 * 14 + 1
for(i in 1:98) seeds[[i]] <- sample.int(1000, 14)
seeds[[99]] <- sample.int(1000, 1)
set.seed(2343)
ptm1 <- proc.time() # Start clock
ctrl <- trainControl(method="cv", 
                     number = 7,
                     seeds=seeds)
modelFitCaretSvm <- train(classe~., 
                                      data=trainSet, 
                                      method="svmRadial",
                                      trControl=ctrl,
                                      tuneLength=14)
ptmDiff <- proc.time() - ptm1 # Stop clock
ptmDiff
modelFitCaretSvm
```

## Choose Model by Predicting with Various Models on Validation Set
### Random Forest Model
First of all, it is noted that "In random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally, during the run,..." (http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#workings) This error is called the out-of-bag (oob) error estimate. But because this is a rather special feature of random forest models, and I have not only used the random forest model, I used the standard method of splitting the data into training and test sets nevertheless. 

```{r predict on valSet with caretRf}
predictionsOnValSetCaretRf <- predict(modelFitCaretRf, newdata=valSet)
cmOnValSetCaretRf <- confusionMatrix(predictionsOnValSetCaretRf, valSet$classe)
cmOnValSetCaretRf
accuracyOnValSetCaretRf <- cmOnValSetCaretRf$overall['Accuracy']
```

### Support Vector Machine Model

```{r predict on vatSet with caretSvm}
predictionsOnValSetCaretSvm <- predict(modelFitCaretSvm, newdata=valSet)
cmOnValSetCaretSvm <- confusionMatrix(predictionsOnValSetCaretSvm, valSet$classe)
cmOnValSetCaretSvm
accuracyOnValSetCaretSvm <- cmOnValSetCaretSvm$overall['Accuracy']
```

### Comparison of both models
```{r accuracy comparison}
accuracyOnValSetCaretRf
accuracyOnValSetCaretSvm
```

We saw that by using the higher tuneLength in the SVM model, the accuracy of this model almost matched the one of the random forest, but even then it was still a bit behind. The random forest model had a very high accuracy already in the second step. 

## Apply Final Model on Test Data Set
Among the two models I tried, random forest was better, on the training set as well as on the validation set. Therefore I will once more calculate the accuracy on an unknown data set, the test data set: 
```{r predict on testSet with caretRf}
predictionsOnTestSetCaretRf <- predict(modelFitCaretRf, newdata=testSet)
cmOnTestSetCaretRf <- confusionMatrix(predictionsOnTestSetCaretRf, testSet$classe)
cmOnTestSetCaretRf
accuracyOnTestSetCaretRf <- cmOnTestSetCaretRf$overall['Accuracy']
```

### Out of sample error
```{r out of sample error of final model}
accuracyOnTestSetCaretRf
outOfSampleError <- 1 - accuracyOnTestSetCaretRf
outOfSampleError
```
The accuracy on the validation set was only 0.1% different from that of the test set, wich is is a good sign that the model will deal well with unknown data and was not overfitted to the training data set. The predicted out of sample error is `r outOfSampleError*100` %. 

## Predict on Quizz Data

Load quizz data set:
```{r read in quizz data set}
quizzDataSet <- read.csv(file="pml-testing.csv", header=TRUE, sep=",", na.strings=c("NA", "", "#DIV/0!")) 
```
No useless variables were removed, as they do not "hurt". I note that normally, any preprocessing that was previously done on the training set, should equally be applied to any other validation, test, or quizz data set. This is especially true for dealing with NAs and NANs, and preprocessing such as box cox transformation. 


```{r predict on quizzSet with caretRf}
predictionsOnQuizzSetCaretRf <- predict(modelFitCaretRf, newdata=quizzDataSet)
predictionsOnQuizzSetCaretRf
answers <- predictionsOnQuizzSetCaretRf
```

The following script was supplied in the assignment tasks. It writes single text files for each of the quizz data set rows. In each text file, the class will be written (A, B, C, D or E)
```
pml_write_files = function(x){
    n = length(x)
    for(i in 1:n){
        filename = paste0("problem_id_",i,".txt")
        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
}

pml_write_files(answers)
```
It is noted that all the test set data were classified correctly!

